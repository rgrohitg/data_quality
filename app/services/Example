import logging
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import types as T
from pyspark.sql.functions import lit, monotonically_increasing_id
from soda.scan import Scan
from soda.common.logs import configure_logging
from soda.sampler.sampler import Sampler
from soda.sampler.sample_context import SampleContext
from app.models.models import ValidationResult
from app.logger_config import logger
from decimal import Decimal

SODA_TO_PYSPARK_TYPE_MAP = {
    'string': T.StringType(),
    'integer': T.IntegerType(),
    'long': T.LongType(),
    'float': T.FloatType(),
    'double': T.DoubleType(),
    'boolean': T.BooleanType(),
    'StringType': T.StringType(),
    'IntegerType': T.IntegerType(),
    'LongType': T.LongType(),
    'FloatType': T.FloatType(),
    'DoubleType': T.DoubleType(),
    'BooleanType': T.BooleanType(),
    # Add more types as needed
}

class CustomSampler(Sampler):
    def __init__(self, spark: SparkSession):
        super().__init__()
        self._spark = spark
        self.retrieved_df = None
        self.failed_rows = []

    def store_sample(self, sample_context: SampleContext):
        rows = sample_context.sample.get_rows()
        if rows:
            schema = T.StructType(
                [T.StructField(name=col.name, dataType=SODA_TO_PYSPARK_TYPE_MAP[col.type])
                 for col in sample_context.sample.get_schema().columns
                 ]
            )
            print("schema:", schema)

            # Add check_name to the schema
            schema = schema.add(T.StructField("check_name", T.StringType(), True))

            # Convert floats to Decimal for saving to DynamoDB but keep as float for PySpark DataFrame
            def to_float(value):
                if isinstance(value, Decimal):
                    return float(value)
                return value

            check_name = sample_context.check_name
            extended_rows = [
                tuple(to_float(value) for value in row) + (check_name,)
                for row in rows
            ]
            print("CHECK NAME:", check_name)

            # Create DataFrame with PySpark types
            self.retrieved_df = self._spark.createDataFrame(extended_rows, schema)
            self.retrieved_df.show(truncate=False)  # Display DataFrame for debugging

            columns = [field.name for field in schema.fields]
            for row in extended_rows:
                failed_row = {columns[i]: value for i, value in enumerate(row)}
                self.failed_rows.append(failed_row)
        else:
            logger.info("No rows found in the sample.")

    def get_retrieved_df(self):
        return self.retrieved_df

    def get_failed_rows(self):
        return self.failed_rows

    def convert_for_dynamodb(self):
        if self.retrieved_df is not None:
            def convert_row(row):
                return {k: (Decimal(v) if isinstance(v, float) else v) for k, v in row.items()}

            dynamodb_rows = [convert_row(row) for row in self.failed_rows]
            return dynamodb_rows
        return []
