import logging
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import types as T
from pyspark.sql.functions import lit, monotonically_increasing_id
from soda.scan import Scan
from soda.common.logs import configure_logging
from soda.sampler.sampler import Sampler
from soda.sampler.sample_context import SampleContext
from app.models.models import ValidationResult
from app.logger_config import logger
from decimal import Decimal

SODA_TO_PYSPARK_TYPE_MAP = {
    'string': T.StringType(),
    'integer': T.IntegerType(),
    'long': T.LongType(),
    'float': T.FloatType(),
    'double': T.DoubleType(),
    'boolean': T.BooleanType(),
    'StringType': T.StringType(),
    'IntegerType': T.IntegerType(),
    'LongType': T.LongType(),
    'FloatType': T.FloatType(),
    'DoubleType': T.DoubleType(),
    'BooleanType': T.BooleanType(),
    # Add more types as needed
}

class CustomSampler(Sampler):
    def __init__(self, spark: SparkSession):
        super().__init__()
        self._spark = spark
        self.retrieved_df = None
        self.failed_rows = []

    def store_sample(self, sample_context: SampleContext):
        rows = sample_context.sample.get_rows()
        if rows:
            schema = T.StructType(
                [T.StructField(name=col.name, dataType=SODA_TO_PYSPARK_TYPE_MAP[col.type])
                 for col in sample_context.sample.get_schema().columns
                 ]
            )
            print("schema:", schema)

            # Add check_name to the schema
            schema.add(T.StructField("check_name", T.StringType(), True))

            # Append check_name to each row and convert floats to Decimal
            check_name = sample_context.check_name
            extended_rows = [
                tuple(Decimal(value) if isinstance(value, float) else value for value in row) + (check_name,)
                for row in rows
            ]
            print("CHECK NAME:", check_name)

            # Create DataFrame
            self.retrieved_df = self._spark.createDataFrame(extended_rows, schema)
            self.retrieved_df.show(truncate=False)  # Display DataFrame for debugging

            columns = [field.name for field in schema.fields]
            for row in extended_rows:
                failed_row = {columns[i]: value for i, value in enumerate(row)}
                self.failed_rows.append(failed_row)
        else:
            logger.info("No rows found in the sample.")

    def get_retrieved_df(self):
        return self.retrieved_df

    def get_failed_rows(self):
        return self.failed_rows
